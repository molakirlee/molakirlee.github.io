---
layout:     post
title:      "python 数据分析处理"
subtitle:   ""
date:       2023-12-21 22:22:00
author:     "XiLock"
header-img: "img/post-background/bamboo3.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 《斤竹精舍·游艺集》
    - Machine_Learning
    - 2023


---

### [归一化 vs 标准化 定量的分析](https://www.afenxi.com/66268.html)
内容很干，这里只列举一下结论：
1. 尝试多种缩放方法可以显著提高你在分类任务上的得分，即使你调整了超参数。因此，你应该将缩放方法视为模型的一个重要超参数。
1. 缩放方法对不同分类器的影响不同。基于距离的分类器，如SVM、KNN和MLP(神经网络)，极大地受益于缩放。但是，即使树(CART、RF)对某些缩放方法是不可知的，也可以从其他方法中获益。
1. 了解模型预处理方法背后的数学原理是理解结果的最佳方法。(例如，树是如何工作的，以及为什么一些缩放方法没有影响它们)。如果你的模型是随机森林，而你不知道如何应用StandardScaler，那么它还可以为您节省大量时间。
1. 像PCA这样的预处理方法已知可以从缩放中获益，确实也是这样。当没有的时候，可能是由于PCA的分量参数个数设置不当、数据中的异常值或缩放方法选择不当。


1. 数据标准化 StandardScaler (基于特征矩阵的列，将属性值转换至服从正态分布) 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下，常用与基于正态分布的算法，比如回归。【业界的首选】
1. 数据归一化 MinMaxScaler （区间缩放，基于最大最小值，将数据转换到0,1区间上的）提升模型收敛速度，提升模型精度 常见用于神经网络
1. Normalizer （基于矩阵的行，将样本向量转换为单位向量） 其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，常见用于文本分类和聚类、logistic回归中也会使用，有效防止过拟合

###### 为什么需要做标准化或者归一化？
a. 目的是使其不同尺度之间的特征具有可比性，同时不改变原始数据的分布。例如y=x1+x2+7是一个简单的线性函数，如果x1的尺度为[0,1]，x2的尺度为[0,10000]，那么x2的变动将会产生更大的影响。这个时候就要对特征进行缩放，也就是标准化或者规范化。简单来说就是让模型不会偏向于某一个特征。
b. 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时，归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。

###### 哪些数据不能做特征缩放？
稀疏数据不能进行特征缩放，因为特征缩放相当于对原数据进平移和缩放。而对于稀疏数据来说，数据中含有大量的0，一旦进行平移，那么稀疏特征将会变为密集特征，这会产生很大的影响，比如特征向量中包含没有在文章中出现的所有单词，那么当它变为密集向量的时候，特征的意义会发生巨大的改变。

###### 什么时候Standardization，什么时候Normalization？
a. 如果对处理后的数据范围有严格要求，那肯定是归一化；
b. 如果无从下手，可以直接使用标准化；
c. 如果数据不稳定，存在极端的最大最小值，不要用归一化。
d. 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。
PS：PCA中标准化表现更好的原因可以参考(PCA标准化)

###### 所有情况都应当Standardization或Normalization么?
a. 当原始数据不同维度特征的尺度(量纲)不一致时，需要标准化步骤对数据进行标准化或归一化处理，反之则不需要进行数据标准化。
b. 也不是所有的模型都需要做归一的，比如模型算法里面有没关于对距离的衡量，没有关于对变量间标准差的衡量。比如决策树，他采用算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的；
c. 另外，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。


### 平滑处理
1. Savitzky-Golay滤波器实现曲线平滑
1. 插值法对折线进行平滑曲线处理
1. 基于Numpy.convolve:实现滑动平均滤波
1. 数据平滑处理--log()和exp()函数

参考资料：
1. [python 数据、曲线平滑处理——方法总结](https://blog.csdn.net/weixin_42782150/article/details/107176500)

![](/img/wc-tail.GIF)
