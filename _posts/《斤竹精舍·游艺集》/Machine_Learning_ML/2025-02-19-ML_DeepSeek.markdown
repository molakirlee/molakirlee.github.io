---
layout:     post
title:      "ML DeepSeek"
subtitle:   ""
date:       2025-02-19 23:50:00
author:     "XiLock"
header-img: "img/post-background/bamboo3.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 《斤竹精舍·游艺集》
    - Machine_Learning
    - 2025


---
### 本地部署deepseek模型
1. [本地部署deepseek模型保姆级教程](https://blog.lovefc.cn/archives/start.html)

### 什么是大模型--[DeepSeek究竟创新了什么？](https://mp.weixin.qq.com/s/UhEgCIwRxCwucnFUDbWO2g)
1. 在人工智能领域，大模型是指基于深度学习框架构建的、拥有海量参数并在大规模数据上进行预训练的深度学习模型。具有参数规模庞大、泛化能力强大、多模态处理能力等特点。
1. 大模型的理论基础是神经网络，这是一种试图让计算机摹仿人脑来工作的理论，该理论和人工智能同时发端，但头40年都不是主流。20世纪80年代中后期，多层感知机模型和反向传播算法得到完善，神经网络理论才有了用武之地。
1. 神经网络理论后来发展为深度学习理论，但到此为止，都是小模型时代，DBN和RNN的参数量通常是几万到几百万，CNN参数量最大，也只有几亿。因此只能完成专门任务，比如基于CNN架构的谷歌AlphaGo，打败了顶尖人类围棋手柯洁和李世石，但它除了下围棋啥也不会。2014年，开发AlphaGo的谷歌DeepMind团队首次提出“注意力机制”。2017年，谷歌提出完全基于注意力机制的Transformer架构，开启大模型时代。 -- 
1. 如上世纪90年代后“深度学习-大模型”路线颠覆了人工智能头几十年的“规则系统-专家系统”路线，“深度学习-大模型”路线也有可能被颠覆，只是我们现在还看不到谁会是颠覆者。
1. 谷歌在大模型时代一路领先，但这几年站在风口上的并不是谷歌，而是2015年才成立的OpenAI，它的各类大模型一直被视为业界顶流，被各路追赶者用来对标。这说明在人工智能领域，看似无可撼动的巨头，其实并非无法挑战。人工智能技术虽然发展了80年，但真正加速也就最近十几年，进入爆发期也就最近两三年，后来者始终有机会。DeepSeek公司2023年7月才成立，它的母体幻方量化成立于2016年2月，也比OpenAI年轻。人工智能就是一个英雄出少年的行业

### DeepSeek-V3架构特点

1. 专家团（MoE架构）；混合专家技术，用更细粒度的专家和共享专家提高训练效率，并且动态调整专家间的工作量均衡。
1. 多头潜在注意力（MLA）：让模型只关注信息中的重要部分，通过压缩注意力机制减少需要处理的信息量，提高效率，不会被不重要的细节分散注意力。
1. 无辅助损失的负载平衡策略：确保专家间工作量均衡，不依赖额外的损失项
1. 多令牌预测训练目标：提高模型的预测能力和数据效率

训练设备：
1. DeepSeek-V3是在拥有2048个NVIDIA H800 GPU的超级计算机上进行训练的。 这些GPU通过NVLink和NVSwitch在单个节点内连接。
1. 节点之间则通过InfiniBand（IB）连接，形成了一个强大的分布式计算网络。

训练框架/算法：
1. DualPipe算法和计算通信重叠。就像两组工人，一组加工零件，一组准备材料。如果他们不同步，加工好的零件就会堆积。 DeepSeek-V3的DualPipe算法让这两组工人的工作节奏同步，一边加工零件，一边准备材料，这样就没有等待时间，生产过程更流畅。
1. 高效实现跨节点全对全通信。你可以想象一个大工厂的不同车间需要共享信息。DeepSeek-V3通过高效的通信技术，确保不同“车间”（计算节点）之间的信息能快速共享，就像建立了一个快速的信息传递网络。

精炼： FP8，通过五个步骤用更小的数字代替原来的大数字，让计算机更快地做计算，同时节省电。比如在关键的地方会用更精确的大数字（FP32）来确保质量，比如：矩阵乘法，这就像在做精细活儿时，在关键步骤用上好工具，其他时候用差点的也没事。在训练过程中，DeepSeek-V3还会用FP8存储中间结果，节省更多的内存空间。这就像整理东西时，不用把所有东西都放在显眼的地方，而是合理地收纳起来，需要时再拿出来。DeepSeek-V3在实际使用时也会根据情况来决定用不用FP8，这样就能在保证效果的同时，让模型跑得更快，更省资源。

预训练：
1. 数据建设：14.8万亿个高质量的数据点来训练，这些数据覆盖了很多不同的领域和语言，这样模型就能学到很多不同的知识。
1. 超参数调整：在训练开始之前，得设置一些重要的参数，比如学习率。DeepSeek-V3会仔细挑选这些参数，让模型能以最好的方式学习。
1. 长上下文扩展：DeepSeek-V3用了一些特别的技术，比如YaRN，来增加模型能处理的文本长度，从4K字节增加到128K字节。这样，模型就能理解更长的文章和故事了。
1. 评估基准：DeepSeek-V3会在各种测试上进行评估，比如MMLMU-Pro、GPQA-Diamond等，确保模型在不同的任务上都能表现得很好。
1. 消融研究：DeepSeek-V3会做很多实验，看看哪些方法最管用。比如研究无辅助损失的负载平衡策略，找出哪些技术最能提高模型的性能等。
1. 辅助无损耗平衡策略：模型通过动态调整，使得每个专家的工作量更加均衡，而不是通过辅助损失来强制平衡。如此一来，预训练阶段就能吸收和处理很多信息，学会理解和生成文本，为后面的训练打下坚实的基础。

后训练：
1. 监督微调（Supervised Fine-Tuning，SFT）：DeepSeek团队为模型准备了150万个实例的特别训练集，就像是一本包含各种生活场景的百科全书。每个训练集都是精心设计，确保模型能学会在不同情况下应该怎么处理。
1. 强化学习（Reinforcement Learning，RL）：对于那些需要逻辑和计算的数据，比如数学问题或者编程挑战，团队用了一个已经训练好的模型（DeepSeek-R1）来生成例子。虽然这些例子通常很准确，但有时可能太复杂或者格式不规范。所以，团队的目标是让数据既准确又容易理解。这些专家模型就像专业的老师，负责教模型如何在特定领域做得更好。在训练过程中，他们会创造两种类型的例子：一种是直接的问题和答案，另一种加上了“系统提示”的问题、答案和R1模型的响应。这些系统提示就像教学大纲，指导模型如何给出有深度和经过验证的答案。在“强化学习”阶段，模型会尝试不同的回答，根据效果得到奖励或惩罚。通过这个过程，模型就学会了给出更好的答案；最后，团队会用“拒绝采样”的方法挑选最好的示例，用于最终模型的训练，这确保了用于模型学习的数据既准确又容易理解。对于非推理数据，比如：写故事或者角色扮演，团队用了另一个模型（DeepSeek-V2.5）来生成回答，然后让人工检查这些回答是否准确和合适。这两个步骤，报告中称之为“评价标准”。
1. 在训练期间，每个序列都由多个样本组成，但他们采用了“样本屏蔽策略”，确保示例相互独立，这是一种“开放评估”的模型。
1. DeepSeek团队对DeepSeek-V3-Base进行了两个时期的微调，采用了从5×10-6到1×10-6的“余弦衰减学习率调度”。

![](/img/wc-tail.GIF)
