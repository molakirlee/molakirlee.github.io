---
layout:     post
title:      "ML 高斯过程·贝叶斯优化"
subtitle:   ""
date:       2025-01-05 06:58:00
author:     "XiLock"
header-img: "img/post-background/bamboo3.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 《斤竹精舍·游艺集》
    - Machine_Learning
    - 2025


---

### 前言
贝叶斯优化是一种求解函数最优值的算法，它最普遍的使用场景是在机器学习过程中对超参数进行调优。贝叶斯优化算法的核心框架是SMBO (Sequential Model-Based Optimization)，而贝叶斯优化（Bayesian Optimization）狭义上特指代理模型为高斯过程回归模型的SMBO。

### SMBO
1. SMBO是一套优化框架，也是贝叶斯优化所使用的核心框架。它有两个重要组成部分：代理模型、优化略略
1. 代理模型（surrogate model），用于对目标函数进行建模。代理模型通常有确定的公式或者能计算梯度，又或者有已知的凹凸性、线性等特性，总之就是更容易用于优化。更泛化地讲，其实它就是一个学习模型，输入是所有观测到的函数值点，训练后可以在给定任意x的情况下给出对f(x)的估计。
1. 优化策略（optimization strategy），决定下一个采样点的位置，即下一步应在哪个输入x处观测函数值f(x)。通常它是通过采集函数（acquisition function） 来实现的
1. 采集函数通常是一个由代理模型推出的函数，它的输入是可行集（feasible set）A上的任意值，输出值衡量了每个输入x有多值得被观测。通常会从以下两方面考虑：有多大的可能性在x处取得最优值；评估x是否能减少贝叶斯统计模型的不确定性。

### 代理模型
1. 代理模型可以有很多，高斯过程、随机森林……等等。其中，贝叶斯优化（Bayesian Optimization） 狭义上特指代理模型为高斯过程回归模型的SMBO。
1. 随机过程（Stochastic/Random Process）可以理解为一系列随机变量的集合。更具体地说，它是概率空间上的一族随机变量{X(t),t∈T}， 其中是t参数，而T又被称作索引集（index set），它决定了构成随机过程的随机变量的个数，大部分情况下，随机变量都是无限个的。
1. 随机过程有时也被称为随机函数（Random Function），因为它也可以被理解为函数值是随机变量的函数。
1. 随机过程的直观含义是我们在 t=0 时刻（此时此刻）来考虑未来每个时间点会发生的情况。例如，当我们把t解释为时间，而X(t)解释为粒子位置时，随机过程就可以被直观地理解为粒子的一种随机运动，它在每个时刻下的位置都是随机的，并且不同时刻t1和t2对应的X(t1)和X(t2)是相关的，它们的相关性由协方差cov[X(t1),X(t2)]定义。
1. 高斯过程（Gaussian Process）是一类随机过程{F(x),x∈A}，它的任意n维分布{F(x1),...,F(xn)}（n也是任意的）都服从多元正态分布。正如一个正态分布可以通过指定均值和方差来确定，一个高斯过程可以通过指定均值函数 m(x) 和协方差函数 K(x,x′)唯一确定，具体公式详见参考资料。
1. 均值函数定义了每个索引x对应的随机变量（同时也是正态分布变量）F(x)的均值；而协方差函数不仅定义了每个索引的方差K(x,x′)，还定义了任意两个索引x1、x2对应的随机变量F(x1)F(x1)F(x2)之间的相关性K(x1,x2)。在高斯过程模型里，协方差函数也被称作核函数（Kernel function）。

### 高斯过程回归
1. 建模前，我们需要事先指定好均值函数 m(x) 和协方差函数 K(x,x′)，它定义了高斯过程F(x)的先验分布。
1. 有一些参数会存在于均值函数和协方差函数中。前面提到的正态分布拟合就像是一种简化：m(x)=μ，K(x,x′)=σ2，这里μ 和σ可以被理解为是两个参数。
1. 选择t个采样索引x1,...,xt（简记为x_1:t），得到目标函数的观测值f(x1),...,f(xt)（简记为f(x_1:t)），它们同样也是高斯过程里对应的随机变量F(x1),...,F(xt) F(x1),...,F(xt)F(x_1:t)）的观测值。
1. 根据观测值调整均值函数和协方差函数里的参数，由此确定最终高斯过程的样子，从而完成对函数f(x)的拟合。
1. 高斯过程回归模型里的参数（主要是均值函数和核函数里的参数），是根据观测到的数据自动地“学习”出来的。“学习”的方法就是最大后验估计（MAP，Maximum A Posterior estimate），选择在观测值已知的情况下最可能的参数值。



### 参考资料
1. [一文详解贝叶斯优化（Bayesian Optimization）原理](https://www.cnblogs.com/milliele/p/17782631.html)
1. [范叶亮;贝叶斯优化 (Bayesian Optimization)](https://leovan.me/cn/2020/06/bayesian-optimization/)


![](/img/wc-tail.GIF)
